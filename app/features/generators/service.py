import asyncio
import logging
import os
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime

import torch
from PIL import Image

from app.model_manager import model_manager_service
from app.services import device_service, image_service, styles_service
from app.socket import socket_service
from config import GENERATED_IMAGES_FOLDER, GENERATED_IMAGES_STATIC_FOLDER

from .constants import DEFAULT_NEGATIVE_PROMPT
from .schemas import (
	GeneratorConfig,
	ImageGenerationItem,
	ImageGenerationResponse,
	ImageGenerationStepEndResponse,
)

logger = logging.getLogger(__name__)


class GeneratorService:
	def __init__(self):
		self.executor = ThreadPoolExecutor()

	@property
	def get_random_seed(self):
		return int(torch.randint(0, 2**32 - 1, (1,)).item())

	def get_seed(self, seed: int):
		random_seed = None

		if seed != -1:
			random_seed = seed
			torch.manual_seed(seed)
			logger.info(f'Using random seed: {seed}')
		else:
			random_seed = self.get_random_seed
			torch.manual_seed(random_seed)
			logger.info(f'Using auto-generated random seed: {random_seed}')

		if device_service.is_available:
			if device_service.is_cuda:
				torch.cuda.manual_seed(random_seed)
			elif device_service.is_mps:
				torch.mps.manual_seed(random_seed)

		return random_seed

	def apply_hires_fix(self, hires_fix: bool):
		if hires_fix:
			logger.warning(
				'Hires fix requested, but not fully implemented in this MVP. '
				'Generating directly at requested resolution.'
			)

	def is_nsfw_content_detected(self, output) -> list[bool]:
		logger.info(f'Checking for NSFW content in the generated images: {output.get("nsfw_content_detected")}')

		if output.get('nsfw_content_detected'):
			logger.warning('NSFW content detected')
			return output.get('nsfw_content_detected')

		return [False] * len(output.get('images', []))

	def generate_file_name(self):
		"""Generate a unique file name based on the current timestamp."""
		return datetime.now().strftime('%Y%m%d_%H%M%S_%f')

	def save_image(self, image):
		if not image:
			logger.error('No image was generated by the pipeline.')
			raise ValueError('Failed to generate any image.')

		file_name = self.generate_file_name()
		save_path = os.path.join(GENERATED_IMAGES_FOLDER, f'{file_name}.png')
		static_path = os.path.join(GENERATED_IMAGES_STATIC_FOLDER, f'{file_name}.png')

		image.save(save_path)

		logger.info(f'Generated image saved to: {save_path}')

		return static_path, file_name

	def latents_to_rgb(self, latents):
		weights = (
			(60, -60, 25, -70),
			(60, -5, 15, -50),
			(60, 10, -5, -35),
		)

		weights_tensor = torch.t(torch.tensor(weights, dtype=latents.dtype).to(latents.device))
		biases_tensor = torch.tensor((150, 140, 130), dtype=latents.dtype).to(latents.device)
		rgb_tensor = torch.einsum('...lxy,lr -> ...rxy', latents, weights_tensor) + biases_tensor.unsqueeze(
			-1
		).unsqueeze(-1)
		image_array = rgb_tensor.clamp(0, 255).byte().cpu().numpy().transpose(1, 2, 0)

		return Image.fromarray(image_array)

	def callback_on_step_end(self, pipe, current_step, timestep, callback_kwargs):
		logger.info(f'Callback on step end: current_step={current_step}, timestep={timestep}')

		latents = callback_kwargs['latents']

		for index, latent in enumerate(latents):
			image = self.latents_to_rgb(latent)
			image_base64 = image_service.to_base64(image)

			logger.info(f'Generated image for current_step {current_step}, index {index}')

			socket_service.image_generation_step_end(
				ImageGenerationStepEndResponse(
					current_step=current_step,
					image_base64=image_base64,
					index=index,
					timestep=timestep,
				)
			)

		return callback_kwargs

	async def generate_image(self, config: GeneratorConfig):
		logger.info(f'Received image generation request: {config}')

		pipe = model_manager_service.pipe

		if pipe is None:
			logger.warning('Attempted to generate image, but no model is loaded.')
			raise ValueError('No model is currently loaded')

		try:
			logger.info(
				f"Generating image(s) for prompt: '{config.prompt}' "
				f'with steps={config.steps}, CFG={config.cfg_scale}, '
				f'size={config.width}x{config.height}'
			)

			model_manager_service.set_sampler(config.sampler)

			self.apply_hires_fix(config.hires_fix)

			random_seed = self.get_seed(config.seed)

			# Apply styles to the prompt
			positive_prompt, negative_prompt = styles_service.apply_styles(
				config.prompt,
				config.styles,
			)
			final_positive_prompt = positive_prompt
			final_negative_prompt = negative_prompt or DEFAULT_NEGATIVE_PROMPT

			logger.info(f'Positive prompt after clipping: {final_positive_prompt}')
			logger.info(f'Negative prompt after clipping: {final_negative_prompt}')

			# Run the image generation in a separate thread to avoid blocking
			# the event loop, especially for long-running tasks like image generation.
			# This is necessary because the pipeline may not be fully async-compatible.
			# Using ThreadPoolExecutor to run the blocking code in a separate thread.
			logger.info('Starting image generation in a separate thread.')

			loop = asyncio.get_event_loop()

			output = await loop.run_in_executor(
				self.executor,
				lambda: pipe(
					prompt=final_positive_prompt,
					negative_prompt=final_negative_prompt,
					num_inference_steps=config.steps,
					guidance_scale=config.cfg_scale,
					height=config.height,
					width=config.width,
					generator=torch.Generator(device=pipe.device).manual_seed(random_seed),
					num_images_per_prompt=config.number_of_images,
					callback_on_step_end=self.callback_on_step_end,
					callback_on_step_end_tensor_inputs=['latents'],
				),
			)

			logger.info(f'Image generation completed successfully: {output}')

			nsfw_content_detected = self.is_nsfw_content_detected(output)

			generated_image = output.get('images', [])
			items: list[ImageGenerationItem] = []

			for image in generated_image:
				if isinstance(image, Image.Image):
					path, file_name = self.save_image(image)
					items.append(ImageGenerationItem(path=path, file_name=file_name))

			return ImageGenerationResponse(
				items=items,
				nsfw_content_detected=nsfw_content_detected,
			)

		except FileNotFoundError as error:
			logger.error(f'Model directory not found: {error}')
			raise ValueError(f'Model files not found: {error}')
		except torch.cuda.OutOfMemoryError as error:
			logger.error(f'Out of memory error during image generation: {error}')
			raise ValueError(
				'Out of memory error during image generation. '
				'Please try again with fewer images or a smaller resolution.'
			)
		except Exception as error:
			logger.exception(f'Failed to generate image for prompt: "{config.prompt}"')
			raise ValueError(f'Failed to generate image: {error}')


generator_service = GeneratorService()
