import logging
import os
import uuid

import torch
from fastapi import APIRouter, HTTPException, status
from fastapi.responses import FileResponse

from app.routers.generators.services import get_available_samplers
from app.services.model_manager import model_manager
from config import BASE_GENERATED_IMAGES_DIR

from .constants import default_negative_prompt
from .schemas import GenerateImageRequest

logger = logging.getLogger(__name__)
generators = APIRouter(
    prefix='/generators',
    tags=['generators'],
)


@generators.post('/start')
async def start_generation_image(request: GenerateImageRequest):
    """
    Generates an image based on the provided prompt and parameters.
    Returns the first generated image as a file.

    Note: 'hires_fix' is acknowledged but not fully implemented in this MVP.
    Batch generation (batch_count > 1) is currently not fully utilized for the response,
    only 'batch_size' images are generated per prompt, and only the first is returned.
    """

    logger.info(f'Received image generation request: {request}')

    pipe = model_manager.pipe

    if pipe is None:
        logger.warning('Attempted to generate image, but no model is loaded.')

        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail='No model is currently loaded. Please load a model first using /models/load.',
        )

    try:
        random_seed = None

        if request.seed != -1:
            torch.manual_seed(request.seed)

            if torch.cuda.is_available():
                torch.cuda.manual_seed(request.seed)

            logger.info(f'Using random seed: {request.seed}')

            random_seed = request.seed
        else:
            random_seed = int(torch.randint(0, 2**32 - 1, (1,)).item())
            torch.manual_seed(random_seed)

            if torch.cuda.is_available():
                torch.cuda.manual_seed(random_seed)

            logger.info(f'Using auto-generated random seed: {random_seed}')

        logger.info(
            f"Generating image(s) for prompt: '{request.prompt}' "
            f'with steps={request.steps}, CFG={request.cfg_scale}, '
            f'size={request.width}x{request.height}, '
            f'batch_size={request.batch_size}, batch_count={request.batch_count}'
        )

        if request.hires_fix:
            logger.warning(
                'Hires fix requested, but not fully implemented in this MVP. Generating directly at requested resolution.'
            )

        if not request.negative_prompt:
            logger.info(f'Use default negative prompt: {default_negative_prompt}')
            request.negative_prompt = default_negative_prompt

        model_manager.set_sampler(request.sampler)

        generation_output = pipe(
            prompt=request.prompt,
            negative_prompt=request.negative_prompt,
            num_inference_steps=request.steps,
            guidance_scale=request.cfg_scale,
            height=request.height,
            width=request.width,
            generator=torch.Generator(device=pipe.device).manual_seed(
                request.seed if request.seed != -1 else random_seed
            ),
        )

        generated_images_list = generation_output[0]

        # Handle potential safety checker output
        # Some pipelines return 'nsfw_content_detected'
        if (
            hasattr(generation_output, 'nsfw_content_detected')
            and generation_output.nsfw_content_detected is not None
            and any(generation_output.nsfw_content_detected)
        ):
            logger.warning('NSFW content detected. Returning a blank image or error.')

            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail='Generated image flagged as NSFW by safety checker.',
            )

        if not generated_images_list:
            logger.error('No images were generated by the pipeline.')

            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail='Failed to generate any image.',
            )

        os.makedirs(BASE_GENERATED_IMAGES_DIR, exist_ok=True)

        filename = os.path.join(BASE_GENERATED_IMAGES_DIR, f'{uuid.uuid4().hex}.png')

        image = generated_images_list[0]

        image.save(filename)
        logger.info(f'Generated image saved to: {filename}')

        return FileResponse(
            filename, media_type='image/png', filename=os.path.basename(filename)
        )

    except FileNotFoundError as e:
        logger.error(f'Model directory not found: {e}')

        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND, detail=f'Model files not found: {e}'
        )
    except Exception as e:
        logger.exception(f'Failed to generate image for prompt: "{request.prompt}"')

        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f'Failed to generate image: {e}',
        )


@generators.get('/available-samplers')
async def available_samplers():
    """
    Returns a list of available samplers for image generation.
    """

    samplers = get_available_samplers()

    if not samplers:
        logger.warning('No available samplers found.')
        return []

    return samplers
