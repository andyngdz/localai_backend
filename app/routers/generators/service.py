import asyncio
import logging
import os
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime

import torch
from PIL import Image

from app.model_manager import model_manager_service
from app.schemas.generators import (
	GeneratorConfig,
	ImageGenerationEachStepResponse,
	ImageGenerationResponse,
)
from app.services import device_service, image_service, styles_service
from app.socket import SocketEvents, socket_service
from config import GENERATED_IMAGES_DIR, GENERATED_IMAGES_STATIC_DIR

from .constants import DEFAULT_NEGATIVE_PROMPT

logger = logging.getLogger(__name__)


class GeneratorService:
	def __init__(self):
		self.id = None
		self.executor = ThreadPoolExecutor()

	@property
	def get_random_seed(self):
		return int(torch.randint(0, 2**32 - 1, (1,)).item())

	def get_seed(self, seed: int):
		random_seed = None

		if seed != -1:
			torch.manual_seed(seed)

			if device_service.is_available:
				torch.cuda.manual_seed(seed)

			logger.info(f'Using random seed: {seed}')

			random_seed = seed
		else:
			random_seed = self.get_random_seed
			torch.manual_seed(random_seed)

			if device_service.is_available:
				torch.cuda.manual_seed(random_seed)

			logger.info(f'Using auto-generated random seed: {random_seed}')

		return random_seed

	def apply_hires_fix(self, hires_fix: bool):
		if hires_fix:
			logger.warning(
				'Hires fix requested, but not fully implemented in this MVP. Generating directly at requested resolution.'
			)

	def is_nsfw(self, output):
		if hasattr(output, 'nsfw_content_detected'):
			logger.warning('NSFW content detected')
			return True

		return False

	def save_image(self, images):
		if not images:
			logger.error('No images were generated by the pipeline.')
			raise ValueError('Failed to generate any image.')

		os.makedirs(GENERATED_IMAGES_DIR, exist_ok=True)
		file_name = datetime.now().strftime('%Y%m%d_%H%M%S_%f')

		save_path = os.path.join(GENERATED_IMAGES_DIR, f'{file_name}.png')
		static_path = os.path.join(GENERATED_IMAGES_STATIC_DIR, f'{file_name}.png')

		image = images[0]
		image.save(save_path)

		logger.info(f'Generated image saved to: {save_path}')

		return static_path, file_name

	def latents_to_rgb(self, latents):
		weights = (
			(60, -60, 25, -70),
			(60, -5, 15, -50),
			(60, 10, -5, -35),
		)

		weights_tensor = torch.t(torch.tensor(weights, dtype=latents.dtype).to(latents.device))
		biases_tensor = torch.tensor((150, 140, 130), dtype=latents.dtype).to(latents.device)
		rgb_tensor = torch.einsum('...lxy,lr -> ...rxy', latents, weights_tensor) + biases_tensor.unsqueeze(-1).unsqueeze(
			-1
		)
		image_array = rgb_tensor.clamp(0, 255).byte().cpu().numpy().transpose(1, 2, 0)

		return Image.fromarray(image_array)

	def callback_on_step_end(self, pipe, step, timestep, callback_kwargs):
		logger.info(f'Callback on step end: step={step}, timestep={timestep}')

		if self.id:
			latents = callback_kwargs['latents']
			image = self.latents_to_rgb(latents[0])
			image_base64 = image_service.to_base64(image)
			socket_service.emit_sync(
				SocketEvents.IMAGE_GENERATION_EACH_STEP,
				ImageGenerationEachStepResponse(
					id=self.id,
					image_base64=image_base64,
				).model_dump(),
			)

		return callback_kwargs

	async def generate_image(self, id: str, config: GeneratorConfig):
		logger.info(f'Received image generation request: {config}')

		self.id = id
		pipe = model_manager_service.pipe

		if pipe is None:
			logger.warning('Attempted to generate image, but no model is loaded.')

			raise ValueError('No model is currently loaded')

		try:
			logger.info(
				f"Generating image(s) for prompt: '{config.prompt}' "
				f'with steps={config.steps}, CFG={config.cfg_scale}, '
				f'size={config.width}x{config.height}'
			)

			model_manager_service.set_sampler(config.sampler)

			self.apply_hires_fix(config.hires_fix)

			random_seed = self.get_seed(config.seed)

			# Apply styles to the prompt
			positive_prompt, negative_prompt = styles_service.apply_styles(
				config.prompt,
				config.styles,
			)
			final_positive_prompt = positive_prompt
			final_negative_prompt = negative_prompt or DEFAULT_NEGATIVE_PROMPT

			logger.info(f'Positive prompt after clipping: {final_positive_prompt}')
			logger.info(f'Negative prompt after clipping: {final_negative_prompt}')

			# Run the image generation in a separate thread to avoid blocking
			# the event loop, especially for long-running tasks like image generation.
			# This is necessary because the pipeline may not be fully async-compatible.
			# Using ThreadPoolExecutor to run the blocking code in a separate thread.
			logger.info('Starting image generation in a separate thread.')

			loop = asyncio.get_event_loop()

			output = await loop.run_in_executor(
				self.executor,
				lambda: pipe(
					prompt=final_positive_prompt,
					negative_prompt=final_negative_prompt,
					num_inference_steps=config.steps,
					guidance_scale=config.cfg_scale,
					height=config.height,
					width=config.width,
					generator=torch.Generator(device=pipe.device).manual_seed(random_seed),
					callback_on_step_end=self.callback_on_step_end,
					callback_on_step_end_tensor_inputs=['latents'],
				),
			)

			images = output[0]

			is_nsfw = self.is_nsfw(output)

			path, file_name = self.save_image(images)

			return ImageGenerationResponse(path=path, file_name=file_name, is_nsfw=is_nsfw)

		except FileNotFoundError as error:
			logger.error(f'Model directory not found: {error}')

			raise ValueError(f'Model files not found: {error}')
		except Exception as error:
			logger.exception(f'Failed to generate image for prompt: "{config.prompt}"')

			raise ValueError(f'Failed to generate image: {error}')


generator_service = GeneratorService()
