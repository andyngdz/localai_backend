import io
import logging
import os
from datetime import datetime

import torch
from PIL import Image

from app.model_manager import model_manager
from app.routers.websocket import SocketEvents, emit
from app.services import styles_service
from config import BASE_GENERATED_IMAGES_DIR

from .constants import default_negative_prompt
from .schemas import GenerateImageRequest

logger = logging.getLogger(__name__)


class GeneratorsService:
    def __init__(self):
        self.id = None

    def get_seed(self, seed: int):
        random_seed = None

        if seed != -1:
            torch.manual_seed(seed)

            if torch.cuda.is_available():
                torch.cuda.manual_seed(seed)

            logger.info(f'Using random seed: {seed}')

            random_seed = seed
        else:
            random_seed = int(torch.randint(0, 2**32 - 1, (1,)).item())
            torch.manual_seed(random_seed)

            if torch.cuda.is_available():
                torch.cuda.manual_seed(random_seed)

            logger.info(f'Using auto-generated random seed: {random_seed}')

        return random_seed

    def apply_hires_fix(self, hires_fix: bool):
        if hires_fix:
            logger.warning(
                'Hires fix requested, '
                'but not fully implemented in this MVP. Generating directly at requested resolution.'
            )

    def check_nsfw_content(self, output):
        if (
            hasattr(output, 'nsfw_content_detected')
            and output.nsfw_content_detected is not None
            and any(output.nsfw_content_detected)
        ):
            logger.warning('NSFW content detected')

            raise ValueError('Generated image flagged as NSFW by safety checker.')

    def save_image(self, images):
        if not images:
            logger.error('No images were generated by the pipeline.')
            raise ValueError('Failed to generate any image.')

        os.makedirs(BASE_GENERATED_IMAGES_DIR, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')
        filename = os.path.join(BASE_GENERATED_IMAGES_DIR, f'{timestamp}.png')

        image = images[0]
        image.save(filename)

        logger.info(f'Generated image saved to: {filename}')

        return filename

    def latents_to_rgb(self, latents):
        weights = (
            (60, -60, 25, -70),
            (60, -5, 15, -50),
            (60, 10, -5, -35),
        )

        weights_tensor = torch.t(
            torch.tensor(weights, dtype=latents.dtype).to(latents.device)
        )
        biases_tensor = torch.tensor((150, 140, 130), dtype=latents.dtype).to(
            latents.device
        )
        rgb_tensor = torch.einsum(
            '...lxy,lr -> ...rxy', latents, weights_tensor
        ) + biases_tensor.unsqueeze(-1).unsqueeze(-1)
        image_array = rgb_tensor.clamp(0, 255).byte().cpu().numpy().transpose(1, 2, 0)

        return Image.fromarray(image_array)

    async def callback_on_step_end(self, pipe, step, timestep, callback_kwargs):
        latents = callback_kwargs['latents']

        image = self.latents_to_rgb(latents[0])

        with io.BytesIO() as output:
            image.save(output, format='PNG')
            image_bytes = output.getvalue()

        await emit(
            SocketEvents.IMAGE_GENERATION_EACH_STEP, {'image_bytes': image_bytes}
        )

        logger.info('Image at step %d: size=%s', step, image.size)

        return callback_kwargs

    def generate_image(self, id: str, request: GenerateImageRequest):
        logger.info(f'Received image generation request: {request}')

        self.id = id
        pipe = model_manager.pipe

        if pipe is None:
            logger.warning('Attempted to generate image, but no model is loaded.')

            raise ValueError('No model is currently loaded')

        try:
            logger.info(
                f"Generating image(s) for prompt: '{request.prompt}' "
                f'with steps={request.steps}, CFG={request.cfg_scale}, '
                f'size={request.width}x{request.height}'
            )

            model_manager.set_sampler(request.sampler)

            self.apply_hires_fix(request.hires_fix)

            random_seed = self.get_seed(request.seed)

            positive_prompt, negative_prompt = styles_service.apply_styles(
                request.prompt,
                request.styles,
            )
            final_positive_prompt = positive_prompt
            final_negative_prompt = negative_prompt or default_negative_prompt

            logger.info(f'Positive prompt after clipping: {final_positive_prompt}')
            logger.info(f'Negative prompt after clipping: {final_negative_prompt}')

            output = pipe(
                prompt=final_positive_prompt,
                negative_prompt=final_negative_prompt,
                num_inference_steps=request.steps,
                guidance_scale=request.cfg_scale,
                height=request.height,
                width=request.width,
                generator=torch.Generator(device=pipe.device).manual_seed(random_seed),
                callback_on_step_end=self.callback_on_step_end,
                callback_on_step_end_tensor_inputs=['latents'],
            )

            images = output[0]

            self.check_nsfw_content(output)

            filename = self.save_image(images)

            return filename

        except FileNotFoundError as error:
            logger.error(f'Model directory not found: {error}')

            raise ValueError(f'Model files not found: {error}')
        except Exception as error:
            logger.exception(f'Failed to generate image for prompt: "{request.prompt}"')

            raise ValueError(f'Failed to generate image: {error}')


generator_service = GeneratorsService()
